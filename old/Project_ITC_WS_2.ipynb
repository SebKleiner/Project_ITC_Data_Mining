{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGES = 50\n",
    "\n",
    "URL = 'https://old.reddit.com/r/datascience/top/?sort=top&t=year'\n",
    "\n",
    "USER_AGENT_LIST = [\n",
    "\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_data(url, user_agent_list):\n",
    "    \n",
    "    users = {}\n",
    "    \n",
    "    users['title'] = []\n",
    "    users['authors'] = []\n",
    "    users['comments'] = []\n",
    "    users['likes'] = []\n",
    "    users['dislikes'] = []\n",
    "    users['dates'] = []\n",
    "    \n",
    "    for i in range(1, PAGES):\n",
    "        \n",
    "        user_agent = random.choice(user_agent_list)\n",
    "        \n",
    "        headers = {'User-Agent': user_agent}\n",
    "        \n",
    "        html = requests.get(url, headers = headers)\n",
    "        \n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        threads = soup.find_all('span', class_='domain')\n",
    "        \n",
    "        for thread in threads:\n",
    "            if thread not in ['(self.datascience)', 'i.redd.it', 'v.redd.it']:\n",
    "                continue\n",
    "            parent_div = thread.parent.parent.parent.parent\n",
    "        \n",
    "        status_code = html.status_code\n",
    "        \n",
    "        domains, titles, authors, comments, likes, dislikes, dates = ([] for i in range(7))\n",
    "        \n",
    "        attrs = {'class': 'thing', 'data-domain': ['self.datascience', 'i.redd.it', 'v.redd.it']}\n",
    "        \n",
    "        for post in soup.find_all('div', attrs=attrs):\n",
    "            \n",
    "            domains.append(post.attrs['data-domain'])\n",
    "            titles.append(post.find('p', class_='title').text)\n",
    "            try:\n",
    "                authors.append(post.find('a', class_='author').text)\n",
    "            except:\n",
    "                authors.append('Unknown')\n",
    "            comments.append(post.find('a', class_='comments').text.split()[0])\n",
    "            likes.append(post.find('div', attrs={'class': 'score likes'}).text)\n",
    "            dislikes.append(post.find('div', attrs={'class': 'score dislikes'}).text)\n",
    "                \n",
    "            date = post.find('p', attrs={'class': 'tagline'})    \n",
    "            date = date.time.prettify()\n",
    "            dates.append(re.findall(r'\\d{4}\\-\\d{2}\\-\\w+\\:\\d+\\:\\d+\\+\\d+' , date)[0])\n",
    "\n",
    "            \n",
    "        users['title'] += titles\n",
    "        users['authors'] += authors\n",
    "        users['comments'] += comments\n",
    "        users['likes'] += likes\n",
    "        users['dislikes'] += dislikes\n",
    "        users['dates'] += dates\n",
    "        \n",
    "        next_button = soup.find('span', class_='next-button')\n",
    "        \n",
    "        try:\n",
    "            url = next_button.find('a').attrs['href']\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "        time.sleep(2)\n",
    "\n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = html_data(URL, USER_AGENT_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(saved, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
